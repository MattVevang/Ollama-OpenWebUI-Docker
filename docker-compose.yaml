services:
  # CPU version - used when no profile is specified
  ollama-cpu:
    image: ollama/ollama:latest
    container_name: ollama-cpu
    volumes:
      - ollama:/root/.ollama
    networks:
      ollama-network:
        aliases:
          - ollama
    ports:
      - 11434:11434
    profiles: [""]  # Only starts with default profile
    healthcheck:
      test: ["CMD", "curl", "-f", "http://localhost:11434"]
      interval: 10s
      timeout: 5s
      retries: 5
    restart: unless-stopped
    
  # GPU version - only with gpu profile
  ollama-gpu:
    image: ollama/ollama:latest
    container_name: ollama-gpu
    volumes:
      - ollama:/root/.ollama
    networks:
      ollama-network:
        aliases:
          - ollama
    ports:
      - 11434:11434
    profiles: ["gpu"]  # Only starts with gpu profile
    deploy:
      resources:
        reservations:
          devices:
            - driver: nvidia
              count: all
              capabilities: [gpu]
    healthcheck:
      test: ["CMD", "curl", "-f", "http://localhost:11434"]
      interval: 10s
      timeout: 5s
      retries: 5
    restart: unless-stopped

  # WebUI - always starts by default with any profile
  open-webui:
    image: ghcr.io/open-webui/open-webui:main
    container_name: open-webui
    volumes:
      - open-webui:/app/backend/data
    depends_on:
      ollama-cpu:
        condition: service_healthy
        required: false
      ollama-gpu:
        condition: service_healthy
        required: false
      stable-diffusion:
        condition: service_started
        required: false
    networks:
      - ollama-network
    ports:
      - 3000:8080
    environment:
      - 'OLLAMA_BASE_URL=http://ollama:11434'
      - 'WEBUI_SECRET_KEY='
      - 'WEBUI_AUTH=False'
      - 'WEBUI_ADMIN_MODE=True'
      - 'AUTOMATIC1111_BASE_URL=http://stable-diffusion:7860'
      - 'AUTOMATIC1111_API_AUTH=admin:admin123'
      - 'ENABLE_IMAGE_GENERATION=true'
      - 'IMAGE_GENERATION_MODEL=v1-5-pruned-emaonly'
      - 'IMAGE_SIZE=640x800'
    restart: unless-stopped
    profiles: ["", "gpu"]  # Start with either no profile or gpu profile

  # Stable Diffusion API - only with gpu profile
  stable-diffusion:
    build:
      context: .
      dockerfile: Dockerfile.sd
    pull_policy: build
    container_name: stable-diffusion
    profiles: ["gpu"]  # Only runs with gpu profile
    # Run as root initially to set permissions
    user: "0:0"  
    ports:
      - 7860:7860
    environment:
      - NVIDIA_VISIBLE_DEVICES=all
      - NVIDIA_DRIVER_CAPABILITIES=compute,utility
    volumes:
      # Use bind mounts instead of named volumes for better permission handling
      - ./sd-models:/home/sduser/app/models
      - ./sd-outputs:/home/sduser/app/outputs
    deploy:
      resources:
        reservations:
          devices:
            - driver: nvidia
              count: all
              capabilities: [gpu]
    networks:
      - ollama-network
    restart: unless-stopped
    healthcheck:
      test: ["CMD-SHELL", "curl -s -f http://localhost:7860/api/heartbeat || exit 1"]
      interval: 10s
      timeout: 5s
      retries: 20
      start_period: 300s  # Increased startup period to give more time

networks:
  ollama-network:

volumes:
  ollama:
  open-webui: