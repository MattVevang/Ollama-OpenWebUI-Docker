services:
  # CPU version - used when no profile is specified
  ollama-cpu:
    image: ollama/ollama:latest
    container_name: ollama-cpu
    ports:
      - 11434:11434
    volumes:
      - ollama:/root/.ollama
      - ./models.txt:/models.txt
      - ./cpu-entrypoint.sh:/cpu-entrypoint.sh
    entrypoint: ["/bin/sh", "/cpu-entrypoint.sh"]
    tty: true
    restart: unless-stopped
    healthcheck:
      test: ["CMD", "ollama", "list"]
      interval: 10s
      timeout: 5s
      retries: 5
    networks:
      ollama-network:  # Use the same format as the GPU version
        aliases:
          - ollama  # This makes it reachable as "ollama" for WebUI
    profiles: ["", "!gpu"]  # Start when no profile is specified, but not with "gpu"

  # GPU version - only with gpu profile
  ollama-gpu:
    image: ollama/ollama:latest
    container_name: ollama-gpu
    profiles: ["gpu"]  # Only start with --profile gpu
    ports:
      - 11434:11434
    volumes:
      - ollama:/root/.ollama
      - ./models.txt:/models.txt
      - ./gpu-entrypoint.sh:/gpu-entrypoint.sh
    entrypoint: ["/bin/sh", "/gpu-entrypoint.sh"]
    tty: true
    restart: unless-stopped
    healthcheck:
      test: ["CMD", "ollama", "list"]
      interval: 10s
      timeout: 5s
      retries: 5
    networks:
      ollama-network:
        aliases:
          - ollama  # This makes it reachable as "ollama" for WebUI
    environment:
      - NVIDIA_VISIBLE_DEVICES=all
      - NVIDIA_DRIVER_CAPABILITIES=compute,utility
      - HSA_OVERRIDE_GFX_VERSION=10.3.0
      - ROCR_VISIBLE_DEVICES=all
    deploy:
      resources:
        reservations:
          devices:
            - driver: nvidia
              count: all
              capabilities: [gpu]

  # WebUI - always starts by default with any profile
  open-webui:
    image: ghcr.io/open-webui/open-webui:main
    container_name: open-webui
    volumes:
      - open-webui:/app/backend/data
    depends_on:
      ollama-cpu:
        condition: service_healthy
        required: false
      ollama-gpu:
        condition: service_healthy
        required: false
    networks:
      - ollama-network
    ports:
      - 3000:8080
    environment:
      - 'OLLAMA_BASE_URL=http://ollama:11434'
      - 'WEBUI_SECRET_KEY='
      - 'WEBUI_AUTH=False'
      - 'WEBUI_ADMIN_MODE=True'
    restart: unless-stopped
    profiles: ["", "gpu"]  # Start with either no profile or gpu profile

networks:
  ollama-network:
    name: ollama-network

volumes:
  ollama: {}
  open-webui: {}