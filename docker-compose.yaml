services:
  # CPU version - used when no profile is specified
  ollama-cpu:
    image: ollama/ollama:latest
    container_name: ollama-cpu
    hostname: ollama
    ports:
      - 11434:11434
    volumes:
      - ollama:/root/.ollama
      - ./models.txt:/models.txt
      - ./cpu-entrypoint.sh:/cpu-entrypoint.sh
    entrypoint: ["/bin/sh", "/cpu-entrypoint.sh"]
    tty: true
    restart: unless-stopped
    healthcheck:
      test: ["CMD", "ollama", "list"]
      interval: 10s
      timeout: 5s
      retries: 5
    profiles: ["", "!gpu"]

  # GPU version - only with gpu profile
  ollama-gpu:
    image: ollama/ollama:latest
    container_name: ollama-gpu
    hostname: ollama
    profiles: ["gpu"]
    ports:
      - 11434:11434
    volumes:
      - ollama:/root/.ollama
      - ./models.txt:/models.txt
      - ./gpu-entrypoint.sh:/gpu-entrypoint.sh
    entrypoint: ["/bin/sh", "/gpu-entrypoint.sh"]
    tty: true
    restart: unless-stopped
    healthcheck:
      test: ["CMD", "ollama", "list"]
      interval: 10s
      timeout: 5s
      retries: 5
    environment:
      - NVIDIA_VISIBLE_DEVICES=all
      - NVIDIA_DRIVER_CAPABILITIES=compute,utility
      - HSA_OVERRIDE_GFX_VERSION=10.3.0
      - ROCR_VISIBLE_DEVICES=all

  # WebUI - always starts by default with any profile
  open-webui:
    image: ghcr.io/open-webui/open-webui:main
    container_name: open-webui
    volumes:
      - open-webui:/app/backend/data
    depends_on:
      ollama-cpu:
        condition: service_healthy
        required: false
      ollama-gpu:
        condition: service_healthy
        required: false
    ports:
      - 3000:8080
    environment:
      - 'OLLAMA_BASE_URL=http://ollama:11434'
      - 'WEBUI_SECRET_KEY='
      - 'WEBUI_AUTH=False'
      - 'WEBUI_ADMIN_MODE=True'
    restart: unless-stopped
    profiles: ["", "gpu"]

volumes:
  ollama: {}
  open-webui: {}