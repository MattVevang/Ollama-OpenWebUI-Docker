# Ollama-OpenWebUI-Docker
A simple working docker container to run Ollama and OpenWebUI locally

## Goal
Share a simple working docker container that will setup Ollama
and OpenWebUI that just works.
In the initial version this does not require setting up local admin
(or seperate) user accounts and is intended to simply work and let you
interact with the LLM of your choosing (and that your compter can run).

## Reason
Setting this up I came upon many pages that went into the weeds about
Docker commands or Ollama etc and I just wanted something that would work.

## How to use
... will add details

## ToDos
I want to seperate the list of LLM models to a seperate file instead
of having them all listed within the Docker compose file.
